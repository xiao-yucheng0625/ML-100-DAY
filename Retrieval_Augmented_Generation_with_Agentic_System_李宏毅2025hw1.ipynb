{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1i8CpjHZDC6gV9xVq6NFc-RyOF5bm0xSG",
      "authorship_tag": "ABX9TyMciK1IjD0YwBIL3ulUcAQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiao-yucheng0625/ML-Progression-Journal/blob/main/Retrieval_Augmented_Generation_with_Agentic_System_%E6%9D%8E%E5%AE%8F%E6%AF%852025hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment setup"
      ],
      "metadata": {
        "id": "LWNt2v6fix1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 colab 與雲端掛接"
      ],
      "metadata": {
        "id": "DG2NxwKBvsU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will mount your own Google Drive and change the working directory.\n",
        "\n",
        "避免 colab 斷線後，產生的檔案會需要重新產生，與雲端 drive 連接後檔案可以存放在裡面。"
      ],
      "metadata": {
        "id": "EO7L5hpOjasM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mgit9LQiEFT",
        "outputId": "e53958cc-0a4b-4fff-e624-a1cd09d0307c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd [/content/drive/MyDrive/LLM團隊-陳雨澤老師+吳澍齊老師/Coding 庫/2025 ML 李宏毅/ML2025 Homework 1 - Retrieval Augmented Generation with Agents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md0yUKhLkAAN",
        "outputId": "ff4d3976-f1f1-4853-8b39-e20ec2bb5cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '[/content/drive/MyDrive/LLM團隊-陳雨澤老師+吳澍齊老師/Coding 庫/2025 ML 李宏毅/ML2025 Homework 1 - Retrieval Augmented Generation with Agents]'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 下載套件、模型權重以及資料集"
      ],
      "metadata": {
        "id": "1MJ9GeqOv2Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ],
      "metadata": {
        "id": "BiobIIx9oBXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "# 安裝 llama-cpp-python 這個套件，版本是 0.3.4 # 推理引擎\n",
        "# GGUF 全稱為「GGML Unified Format」 # 存放權重\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "# googlesearch-python：用於 Google 搜尋。\n",
        "# bs4：BeautifulSoup4，用來解析 HTML。\n",
        "# charset-normalizer：解決編碼問題。\n",
        "# requests-html：便於處理動態網頁及JavaScript渲染。\n",
        "# lxml_html_clean：用來清理並整理 HTML 文件。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBi-tf8wmu3r",
        "outputId": "f0494bde-427e-4d2c-98ef-b9af1cbe2a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.5.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.1.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下載 Llama3.1-8B 模型與相關檔案（public 與 private dataset）"
      ],
      "metadata": {
        "id": "jAFuutS8qlyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzi-EDaWptI6",
        "outputId": "ea87bbe1-fe1f-4ad0-dcef-6e58d2d98ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 02:05:48--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.16, 18.239.50.103, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1742785551&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjc4NTU1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lRB3LHjt70AjHM7AjN03ntIaBWG4PiXYNU6NCVYLskbfi7sBaaJAUbf0Sdd9O5mLWHtoanpAsNnJXJcVfPF9yIyYb7KKPMLOvREPOpFckRE9cFZIFzP6eeEpxQQo4W8tTw7Ik49Y319gPvX7vPWacz4cAbeJ12lpJWU07JTbQnaxhcqE1K5-NoGobXS3uYI5tt1JLEnOHerrIlsr5JIUpMgrfdaxmYqG-YLyelXvtTuUFaEX2Xr7LXEvyxIuN1PZQMc-egabQ4KNf24B3AfOb4GtvlhCYmqwgw-58iDy7QZC2w71N5433SVWUuWGZsDqGcT%7E27HJYbygn08BHNnTEw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-24 02:05:51--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1742785551&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjc4NTU1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lRB3LHjt70AjHM7AjN03ntIaBWG4PiXYNU6NCVYLskbfi7sBaaJAUbf0Sdd9O5mLWHtoanpAsNnJXJcVfPF9yIyYb7KKPMLOvREPOpFckRE9cFZIFzP6eeEpxQQo4W8tTw7Ik49Y319gPvX7vPWacz4cAbeJ12lpJWU07JTbQnaxhcqE1K5-NoGobXS3uYI5tt1JLEnOHerrIlsr5JIUpMgrfdaxmYqG-YLyelXvtTuUFaEX2Xr7LXEvyxIuN1PZQMc-egabQ4KNf24B3AfOb4GtvlhCYmqwgw-58iDy7QZC2w71N5433SVWUuWGZsDqGcT%7E27HJYbygn08BHNnTEw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.156.60.74, 108.156.60.97, 108.156.60.36, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.156.60.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G) [binary/octet-stream]\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   227MB/s    in 46s     \n",
            "\n",
            "2025-03-24 02:06:37 (178 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-03-24 02:06:37--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-24 02:06:39 (282 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n",
            "--2025-03-24 02:06:39--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15229 (15K) [text/plain]\n",
            "Saving to: ‘private.txt’\n",
            "\n",
            "private.txt         100%[===================>]  14.87K  68.0KB/s    in 0.2s    \n",
            "\n",
            "2025-03-24 02:06:40 (68.0 KB/s) - ‘private.txt’ saved [15229/15229]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 檢查當前環境是否有 CUDA 支援的 GPU 可以使用。"
      ],
      "metadata": {
        "id": "D4rkQXY-rC8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy3Zl4aPonr5",
        "outputId": "c5b0a02c-8d94-4cb1-e400-34d9599d3278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare LLM and LLM utility function"
      ],
      "metadata": {
        "id": "PWO7EKYJsKkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 載入模型權重至 GPU 上。"
      ],
      "metadata": {
        "id": "V9d8puRotcD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\", # 檔案路徑\n",
        "    verbose=False, # 表示不會顯示過多的模型載入過程細節，僅顯示必要資訊。\n",
        "    n_gpu_layers=-1, # 將模型全部層數載入 GPU\n",
        "    n_ctx=16384,    # tokens number # 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqxahHA9reQJ",
        "outputId": "76f67311-9dd9-4cd6-e004-d70852774235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 定義 LLM 產生回覆的函式"
      ],
      "metadata": {
        "id": "UmkNOJu4tvSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(_model: Llama, _messages: list) -> str:\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,\n",
        "        temperature=0,      # 模型輸出的隨機程度\n",
        "        repeat_penalty=2.0,   # 避免模型生成重複的內容或用詞\n",
        "    )[\"choices\"][0][\"message\"][\"content\"] # json # 提取生成內容\n",
        "    return _output"
      ],
      "metadata": {
        "id": "yq3TWovmtvk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Search tool"
      ],
      "metadata": {
        "id": "plwzOov1wOEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 整合 Google search 的工具至 pipeline"
      ],
      "metadata": {
        "id": "dISTI0uQwciQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "引入不同的 Library，常用於網路爬蟲、資料擷取與自動化搜尋任務。"
      ],
      "metadata": {
        "id": "ka0NF4-7yc6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup     # 將 HTML 網頁內容進行解析，方便後續提取特定資料或文字\n",
        "from charset_normalizer import detect # 檢測文字資料的編碼格式\n",
        "import asyncio             # 建立非同步任務，能有效提高網頁爬蟲抓取速度\n",
        "from requests_html import AsyncHTMLSession # 可同時抓取多個網頁，且支援動態網頁內容\n",
        "import urllib3             # 處理 HTTP 請求和響應\n",
        "urllib3.disable_warnings()        # 抑制 HTTPS 請求時發生的安全性警告"
      ],
      "metadata": {
        "id": "OJ2c553hwMNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "worker()：負責抓取單一網頁內容，並檢查網址是不是HTML頁面。"
      ],
      "metadata": {
        "id": "MAAdvYg2yqAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        # 發送HEAD請求檢查頁面類型，若不是HTML則跳過\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        # 如果是HTML，發送GET請求取得頁面內容\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "LlihRFMiw0bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_htmls()：同時非同步地抓取多個網頁內容（呼叫 worker）"
      ],
      "metadata": {
        "id": "5zhFIyffzo0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "YFhRgjTwwwfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "search()：\n",
        "* 執行 Google 搜尋。\n",
        "* 呼叫前兩個函式取得網頁內容\n",
        "* 使用 BeautifulSoup 轉換成文字。"
      ],
      "metadata": {
        "id": "bd3SxKNx0Fjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n"
      ],
      "metadata": {
        "id": "oGz9ydGc00oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    cleaned_results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ],
      "metadata": {
        "id": "yImJ4fRLwv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 測試 LLM 推論的成效"
      ],
      "metadata": {
        "id": "VEjXusJp1-vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from opencc import OpenCC: post-process（後處理）強制轉換成繁體"
      ],
      "metadata": {
        "id": "wZ0kv_EX-dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7MA6tlx_pmB",
        "outputId": "861495ed-7ec9-43c2-c0ee-b635bd1a5a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencc in /usr/local/lib/python3.11/dist-packages (1.1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "system prompt: 設定模型的個性、身分、語氣\n",
        "\n",
        "\n",
        "user prompt: 下指令，請模型完成任務\n",
        "\n"
      ],
      "metadata": {
        "id": "F3V1Xnpq_6Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "from opencc import OpenCC\n",
        "cc = OpenCC('s2t')  # 簡轉繁\n",
        "response = generate_response(llama3, messages)\n",
        "response = cc.convert(response)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AdW2dbS-K7s",
        "outputId": "4a137daa-dc7e-4681-a8a3-1c80436be4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音樂風格從鄉村搖滾發展到流行搖擺，並且她被譽爲當代最成功的女藝人的之一。\n",
            "\n",
            "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大型作品如 《1989》（2014年）、_reputation（）和 _Lover （）。她的歌曲經常探討愛情、友誼及自我成長等主題。\n",
            "\n",
            "泰勒絲獲得了許多獎項，包括13座格萊美獎，並且是史上最快達到百萬銷量的女藝人之一。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 設置 Agents"
      ],
      "metadata": {
        "id": "6yWXXNiaASz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 class LLMAgent()\n",
        "屬性（Attributes）：\n",
        "\n",
        "* role_description（角色描述）：\n",
        "定義這個 agent 的身份。\n",
        "例如：如果你希望這個 agent 是一位歷史專家，可以設定為：\n",
        "「你是一位歷史專家，只根據真實發生過的歷史事件回答問題。若沒有可靠資料來源，請不要作答。」\n",
        "\n",
        "* task_description（任務描述）：\n",
        "定義這個 agent 應該執行的任務。\n",
        "例如：如果你希望 agent 只用「是／否」來回答問題，可以設定為：\n",
        "「請用是或否回答以下問題，不需要說明理由。」\n",
        "\n",
        "* llm（語言模型）：\n",
        "表示這個 agent 使用的是哪一個大型語言模型（LLM）。\n",
        "\n",
        "方法(Method):\n",
        "\n",
        "* inference（推理方法）：\n",
        "這個方法接收一段訊息作為輸入，並回傳語言模型產生的回應。\n",
        "\n",
        "  在進行推理前，訊息會先被格式化成適合語言模型理解的格式。\n",
        "\n",
        "  你也可以在這裡加上全域指令，像是：\n",
        "\n",
        "  「請使用有禮貌的語氣」\n",
        "\n",
        "  「請提供詳細說明」\n",
        "\n",
        "  最後，語言模型會根據這些設定產生回應，並作為輸出回傳。"
      ],
      "metadata": {
        "id": "o0hot5LWAWQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF':\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # \\n distinguish the task descriptions and the user messages\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ],
      "metadata": {
        "id": "VeDei0GO_BsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Design the role description and task description for each agent"
      ],
      "metadata": {
        "id": "Zaam6OBDDAb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一個 question_extraction_agent"
      ],
      "metadata": {
        "id": "TTkkLdFRD7sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，專職提取問題內容，忽略情緒、評論、背景與舉例。僅使用繁體中文回應，不回答問題。\",\n",
        "    task_description=\"請從輸入中提取一個語意完整、語法正確的核心問題。排除無關內容，保留專有名詞與真實問題語意，不改寫原意。\",\n",
        ")"
      ],
      "metadata": {
        "id": "az77IUZFC-tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第二個 keyword_extraction_agent"
      ],
      "metadata": {
        "id": "SLlaLRhfD_BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，專職萃取問題關鍵字。僅使用繁體中文回應，不解釋、不回答。\",\n",
        "    task_description=\"請從問題中擷取最適合搜尋的關鍵字，保留專有名詞與語境詞。以空白分隔字串輸出，禁止說明與改寫。\",\n",
        ")"
      ],
      "metadata": {
        "id": "Memwq2EnDxBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第三個 qa_agent"
      ],
      "metadata": {
        "id": "KDtTZWYUECEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，只根據上下文事實簡潔作答。回答風格客觀中立，僅使用繁體中文。\",\n",
        "    task_description=\"請根據提供的資訊，直接作答。禁止延伸、假設或修飾語，避免不必要空格或鋪陳。\",\n",
        ")"
      ],
      "metadata": {
        "id": "ovuXFqB6DzvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. RAG pipeline"
      ],
      "metadata": {
        "id": "iZJstLRTF6lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ],
      "metadata": {
        "id": "wXbI-e6AGUx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "  # Step 1: 關鍵字提取 Agent 擷取關鍵字\n",
        "  keywords = keyword_extraction_agent.inference(question)\n",
        "  print(f\"✅ Extracted Keywords: {keywords}\")\n",
        "\n",
        "  # Step 2: 問題過濾 Agent 清理問題\n",
        "  Core_problem = question_extraction_agent.inference(question)\n",
        "  print(f\"✅ Core problem: {Core_problem}\")\n",
        "\n",
        "  # Step 3: 關鍵字搜尋\n",
        "  results = await search(keywords)\n",
        "  limited_results = \" \".join([result.get_text()[:1000] for result in results])\n",
        "\n",
        "  # Step 4: 吐出答案\n",
        "  return qa_agent.inference(f\"{limited_results}，{Core_problem}\")"
      ],
      "metadata": {
        "id": "LltkEN1zF1pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 使用設計好的 RAG pipeline 回答資料集的問題"
      ],
      "metadata": {
        "id": "s1kBCqNPIBaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"N56131322\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            Path(f\"./{STUDENT_ID}_{id}.txt\").unlink()\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            Path(f\"./{STUDENT_ID}_{id}.txt\").unlink()\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "milt2CJmIBCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb17e14-8f64-4e59-d99e-5e5e6cb5a4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted Keywords: 虎山雄風飛揚  中國人民大學\n",
            "✅ Core problem: 提取的核心問題：「虎山雄風飛揚」是哪間學校校歌？\n",
            "1 光華國小\n",
            "✅ Extracted Keywords: 2025年 NCC 行政命令 境外郵購 審查費\n",
            "✅ Core problem: 透過境外郵購無線鍑盤、滑鼠及藍芽耳機等自用產品回台的民眾，會被加收多少審查費？\n",
            "2 750元\n",
            "✅ Extracted Keywords: 蘋果  CEO   Steve Jobs\n",
            "✅ Core problem: 提取的核心問題：第一代 iPhone 是由哪位 CEO 發表？\n",
            "3 史蒂夫·乔布斯\n",
            "✅ Extracted Keywords: 托福網路測驗 TOEFL iBT 免修申請分數\n",
            "✅ Core problem: 托福網路測驗 TOEFL iBT 的分數要求\n",
            "4 92分（含）以上\n",
            "✅ Extracted Keywords: Rugby Union 觸地 try 得分\n",
            "✅ Core problem: 觸地 try\n",
            "5 觸地（try）\n",
            "✅ Extracted Keywords: 卑南族  臺東平原   ruvuwa'an\n",
            "✅ Core problem: 卑南族的祖先發源地是ruvuwa'an。\n",
            "6 卑南族\n",
            "✅ Extracted Keywords: 熊仔  饒舌創作歌手 金曲獎 最佳 作詞人 獎 碩班 指導教授\n",
            "✅ Core problem: 熊仔的碩班指導教授為？\n",
            "7 陳宥林\n",
            "✅ Extracted Keywords: 麥克斯韋　電磁感應定律\n",
            "✅ Core problem: 發現電磁感應定律的科學家是詹姆斯·克拉ーク・マ克斯韋爾。\n",
            "8 詹姆斯·克拉ーク・マ克斯韋爾\n",
            "✅ Extracted Keywords: 國立臺灣史前文化博物館 臺鐵車站\n",
            "✅ Core problem: 距離國立臺灣史前文化博物館最近的臺鐵車站為？\n",
            "9 臺東車站\n",
            "✅ Extracted Keywords: 20 加 30\n",
            "✅ Core problem: 核心問題：20+30=?\n",
            "10 50\n",
            "✅ Extracted Keywords: 達拉斯  NBA Luka Doncic 獨行俠隊交易\n",
            "✅ Core problem: Luka Doncic\n",
            "11 庫班表示自己也跟大家一樣目瞪口呆。\n",
            "✅ Extracted Keywords: 2024年 美國 總統 大選\n",
            "✅ Core problem: 2024年美國總統大選\n",
            "12 2024年美國總統大選由共和黨候选人川普（Donald Trump）勝出。\n",
            "✅ Extracted Keywords: Meta Llama-3.2 系列模型 參數量 最小 Billion\n",
            "✅ Core problem: 參數量最小的模型是多少億個参数？\n",
            "13 1B\n",
            "✅ Extracted Keywords: 停修 學期 初選課退 chọn 國立臺灣大學學則\n",
            "✅ Core problem: 提取核心問題：依據國立臺灣大學學則，停修有多嚴格的限制？\n",
            "14 依據國立臺灣大學學則，停修有嚴格的限制：\n",
            "✅ Extracted Keywords: DeepSeek\n",
            "✅ Core problem: DeepSeek公司的母公\n",
            "15 DeepSeek公司的母公是？\n",
            "✅ Extracted Keywords: 2024年 NBA 总冠军隊伍\n",
            "✅ Core problem: 2024年NBA的總冠軍隊伍\n",
            "16 波士顿凯尔特人\n",
            "✅ Extracted Keywords: 碳氫化合物 三鍵\n",
            "✅ Core problem: 碳原子與其他分子的三鍵形成的化合物\n",
            "17 碳氫鍵\n",
            "✅ Extracted Keywords: 圖靈  計算理論\n",
            "✅ Core problem: 提取核心問題：被譽為「計算機科學之父」，提出圖靈机概念、为現代计算理论奠定基础的人是誰？\n",
            "18 阿尔弗雷德·图灵\n",
            "✅ Extracted Keywords: 臺灣  玄天上帝信仰進香中心\n",
            "✅ Core problem: 提取核心問題：臺灣玄天上帝信仰的進香中心位於哪個行政區劃內？\n",
            "19 松柏嶺受天宮 │ 臺灣北極玄 天上帝信仰中心\n",
            "✅ Extracted Keywords: Windows  Microsoft\n",
            "✅ Core problem: 核心問題：Windows 作業系統是哪間科技公司的產品？\n",
            "20 微軟\n",
            "✅ Extracted Keywords: 官將首  臺灣傳統信仰文化陣頭 八家将 福州 傳衍 主神 駕前侍衛 迎 神 活動 押煞\n",
            "✅ Core problem: 官將首的起源\n",
            "21 官將首起源於五福大帝信仰\n",
            "✅ Extracted Keywords: 《咒》邪神 詛 咬 魔\n",
            "✅ Core problem: 提取的核心問題：《咒》的邪神名為？\n",
            "22 没有提供《咒》的邪神名。\n",
            "✅ Extracted Keywords: 短暫交會的旅程就此分岔\n",
            "✅ Core problem: 提取的核心問題是：「短暫交會旅程就此分岔」歌曲中的哪個團體？\n",
            "23 動力火車\n",
            "✅ Extracted Keywords: 卑南族 協會 年祭 聯合年聚\n",
            "✅ Core problem: 卑南族聯合年聚\n",
            "24 2025卑南族聯合年祭\n",
            "✅ Extracted Keywords: GeForce RTX\n",
            "✅ Core problem: 核心問題：最新的輝達顯卡是出到「GeForce RTX 多少」系列？\n",
            "25 GeForce 50系列\n",
            "✅ Extracted Keywords: 大S  台灣\n",
            "✅ Core problem: 提取的核心問題：大S去世時是在哪個國家旅遊？\n",
            "26 大S去世時是在台灣。\n",
            "✅ Extracted Keywords: 艾薩克·牛頓\n",
            "✅ Core problem: 發現萬有引力的科學家是艾薩克·牛頓。\n",
            "27 艾萨克·牛顿\n",
            "✅ Extracted Keywords: 台鵠開示計畫 TAIHUCAIS\n",
            "✅ Core problem: 台鵠開示計畫「TAIHUCAIS」的英文全名。\n",
            "28 台鵠開示是TAIHUCAIS：台灣人文學者對話式知識探勘系統。\n",
            "✅ Extracted Keywords: 電影  台詞\n",
            "✅ Core problem: 提取的核心問題：是出自哪部電影？\n",
            "29 《千与 千寻》\n",
            "✅ Extracted Keywords: 水  氫氧化物\n",
            "✅ Core problem: 水的化學式\n",
            "30 H2O\n",
            "✅ Extracted Keywords: 李宏毅 Hung-yi Lee 台灣大學 電機工程學系 機器 學習 深度 運算 語音 處理\n",
            "✅ Core problem: 提取的核心問題是：李宏毅在台灣大學開設《機器學習》 2023 年春季班中，第15個作業名稱為何？\n",
            "31 無法從提供的資訊中找到李宏毅在台灣大學開設《機器學習》 2023 年春季班中的第15個作業名稱。\n",
            "✅ Extracted Keywords: 臺灣科技大學\n",
            "✅ Core problem: 該獨立學院為何？\n",
            "32 獨立學院\n",
            "✅ Extracted Keywords: BitTorrent 協議  盜版影音節點加入網路 chunk 種子資料交換\n",
            "✅ Core problem: 核心問題：BitTorrent 協議如何確保新節點能從其他種子隨機獲得部分資料？\n",
            "33 BitTorrent 協議使用 Kademlia 分佈式哈希表和 DHT（分散性查找服務）來實現節點之間的資料交換。\n",
            "✅ Extracted Keywords: youtube  英文 sure 摔車\n",
            "✅ Core problem: 提取核心問題：那個是甚麼影片阿？\n",
            "34 影片名是「沒有用的團結力合輯」\n",
            "✅ Extracted Keywords: 戈芬氏鳳頭鸚鵡 奧地利維也納大學獸醫學院 乙醯 醬料 馬鈴薪 沾沁\n",
            "✅ Core problem: 提取的核心問題是：\n",
            "\n",
            "最受戈芬氏鳳頭鸚鵡偏好且深具吸引力的乳酪口味到底是什么？\n",
            "35 藍莓沾醬\n",
            "✅ Extracted Keywords: 桃園 Xpark 水族館 國王企鵝 嘟胖 烏龍茶 產下 宗主名投票\n",
            "✅ Core problem: 提取核心問題：最後這隻企鵝寶嬰兒的名子是什麼？\n",
            "36 桃園Xpark國王企鵝寶宝徴名\n",
            "✅ Extracted Keywords: 運動傷害  物理治療師   國立臺灣大學物理學\n",
            "✅ Core problem: 國立臺灣大學物理治療學系的正常修業年限\n",
            "37 4年\n",
            "✅ Extracted Keywords: 《BanG Dream!》角色的笑聲習慣呼嘿\n",
            "✅ Core problem: 《BanG Dream!》中哪位角色笑聲習慣是「呼嘿 嘻」？\n",
            "38 Roselia\n",
            "✅ Extracted Keywords: 甲斐之虎 日本戰國時代\n",
            "✅ Core problem: 提取的核心問題：日本戰國時代被稱為「甲斐之虎」的人物是誰？\n",
            "39 武田信玄\n",
            "✅ Extracted Keywords: 網路  好評\n",
            "✅ Core problem: 提取核心問題：王肥貓同學最有可能去修哪一門課？\n",
            "40 獲得更多線上評論的6種方法！\n",
            "✅ Extracted Keywords: 《極限體能王SASUKE》 2024年 第42回 首播 日子\n",
            "✅ Core problem: 提取的核心問題是：\n",
            "\n",
            "2024年的第42回《極限體能王SASUKE》在哪一天首播？\n",
            "41 2024年的第42回《極限體能王SASUKE》首播日期尚未公布。\n",
            "✅ Extracted Keywords: 利嘉部落  初鹿 部 落 頭目 漢人\n",
            "✅ Core problem: 出身於利嘉部落，後來成為初鹿頭目的漢人。\n",
            "42 马智礼出生于1887年，大清福建省。\n",
            "✅ Extracted Keywords: 《BanG Dream!》 月刊武士道 跨媒體發行 片頭曲 Ave Mujica\n",
            "✅ Core problem: 提取的核心問題：《BanG Dream! Ave Mujica》的片頭曲是哪一首？\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是「MyGO!!!!!」。\n",
            "✅ Extracted Keywords: Linux作業系統  Linus Torvalds\n",
            "✅ Core problem: Linux作業系統首次發布年份\n",
            "44 1991\n",
            "✅ Extracted Keywords: 卑南族 Likavung\n",
            "✅ Core problem: 提取的核心問題：Likavung 的中文名稱為何？\n",
            "45 利嘉部落（Likavung）\n",
            "✅ Extracted Keywords: 紅茶 全發酵\n",
            "✅ Core problem: 提取的核心問題：紅茶是全發酵還是不完全或半生不熟（部分）黑、綠等其他類型？\n",
            "46 紅茶是全發酵。\n",
            "✅ Extracted Keywords: 真紅眼黑龍 黑魔導\n",
            "✅ Core problem: 真紅眼黑龍與 黑魔導的融合怪獸是「極凜特拉」\n",
            "47 超魔導龍騎士-真紅眼黑兵衛\n",
            "✅ Extracted Keywords: 豐田萌繪 BanG Dream! 角色聲優\n",
            "✅ Core problem: 豐田萌繪在《BanG Dream!》企劃中，擔任哪個角色的聲優？\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任Ringo角色聲優。\n",
            "✅ Extracted Keywords: Rugby Union 9 號球員正式名稱\n",
            "✅ Core problem: 9 號球員的正式名稱為掃劍半back（Scrum-half）。\n",
            "49 橄欖球是一種聯合式運動，兩隊各有15名選手。\n",
            "✅ Extracted Keywords: 行星 太陽系 矮 行 星\n",
            "✅ Core problem: 提取的核心問題：曾被視為太陽系中的行星，最終降格成矮 行 星球是哪一個？\n",
            "50 谷神星\n",
            "✅ Extracted Keywords: 野生動物 動保法 臺灣 行政區\n",
            "✅ Core problem: 提取的核心問題：臺灣最早成立的地方是哪個行政區內？\n",
            "51 臺北市\n",
            "✅ Extracted Keywords: 南投縣集集中特生中心\n",
            "✅ Core problem: 位於南投縣集集中特生中心的改名問題\n",
            "52 農業部生物多樣性研究所\n",
            "✅ Extracted Keywords: Instruction-Following Speech Language Model\n",
            "✅ Core problem: Instruction-Following Speech Language Model\n",
            "53 DeSTA2: Developing Instruction-Following Speech Language Model WithoutSpeechInstruction-Tuning Data\n",
            "✅ Extracted Keywords: 太陽系 行星 體積\n",
            "✅ Core problem: 太陽系中體積最大的行星是金環木衛。\n",
            "54 是的\n",
            "✅ Extracted Keywords: 南島語系  原住民族 語言分類學\n",
            "✅ Core problem: 核心問題：哪一族的語言與其他原住民族在分類學上一般不被視為同一個群？\n",
            "55 南島語系是全球最大的语言家族之一，包含109种语言。\n",
            "✅ Extracted Keywords: 台灣大學  阿宅 PM\n",
            "✅ Core problem: 提取的核心問題是：講出這句話的是誰？\n",
            "56 Evonne Tsai\n",
            "✅ Extracted Keywords: 布農族 阿美족 打招呼用語 原住民族\n",
            "✅ Core problem: 提取的核心問題：「embiyax namu kana」是哪一臺灣原住民族 的打招呼用語？\n",
            "57 樂說族\n",
            "✅ Extracted Keywords: 布農族 鄒족 混居部落\n",
            "✅ Core problem: 提取核心問題：「鄒與布農，永久美麗」這句話相關的部落是哪一個？\n",
            "58 布農\n",
            "✅ Extracted Keywords: 公會 櫃檯小姐 迷宮頭目 冒險者\n",
            "✅ Core problem: 提取的核心問題：女主角隱藏的是甚麼身份？\n",
            "59 女主角隱藏的是公會櫃檯小姐的身份。\n",
            "✅ Extracted Keywords: 卑南族 Tuku 姐妹部落\n",
            "✅ Core problem: 提取核心問題：姊妹 Tuku 創建了哪一個部落？\n",
            "60 射馬干部落\n",
            "✅ Extracted Keywords: 2005 年 《終極一班》 KO榜 首位\n",
            "✅ Core problem: 2005 年播出的電視劇《終極一班》中的「KO榜」首名是？\n",
            "61 《終極一班》中的「KO榜」首名是張棟樑\n",
            "✅ Extracted Keywords: Linux kernel CFS process scheduler  red-black tree\n",
            "✅ Core problem: 核心問題：Linux kernel 的 completely fair scheduler (CFS) 採用了何種資料結構來儲存排程相關資訊？\n",
            "62 紅黑樹（Red Black Tree）\n",
            "✅ Extracted Keywords: 諾曼第登陸 Operation Overlord\n",
            "✅ Core problem: 諾曼第登陸的作戰代號\n",
            "63 霸王行动\n",
            "✅ Extracted Keywords: 《Cytus II》 Body Talk\n",
            "✅ Core problem: 《Cytus II》遊戲中「Body Talk」是哪位角色的歌曲？\n",
            "64 M2U\n",
            "✅ Extracted Keywords: 李琳山教授  國立臺灣大學電機工程學系   史丹佛大学\n",
            "✅ Core problem: 李琳山教授在國立臺灣大學所開設的信號與系統課程，在期末考前後會有一次演講，該學生稱為 \"最後一堂\"。\n",
            "65 李琳山教授  1977年在美國史丹佛大學拿到電機博士，專攻人造衛星通訊。  台美斷交後回國，在母系臺大任教，因為「家不需要理由」；雖然當時台灣絕無研究的人工智慧之環境。\n",
            "✅ Extracted Keywords: 朋友窮 5090 顯卡 RTX LLM VRAM\n",
            "✅ Core problem: 提取核心問題：RTX 5090 的 VRAM 到底是多大？\n",
            "66 無法從提供的資訊中找到 RTX 5090 的 VRAM 大小。\n",
            "✅ Extracted Keywords: 世界棒球12強賽 台灣 中華職業聯盟\n",
            "✅ Core problem: 提取的核心問題：世界棒球12強賽\n",
            "67 2024年世界棒球12強賽\n",
            "✅ Extracted Keywords: 中國四大奇書  奇經八章\n",
            "✅ Core problem: 中國四大奇書\n",
            "68 四大奇書是章回小說《水滸傳》、《三國演義》，\n",
            "✅ Extracted Keywords: 子時 23 點到早上一點\n",
            "✅ Core problem: 提取的核心問題：子時在24小时制中的時間範圍\n",
            "69 23點到1点\n",
            "✅ Extracted Keywords: 作業系統 時限 排程演算法 遲交\n",
            "✅ Core problem: 避免要錯過時限來完成作業的排程演算法。\n",
            "70 非抢占式（Non-preemptive）和预测性排程演算法有以下特点：  1. 非搶先：執行->等待(不可預測) 2.. 等候時間(Waiting Time) = Pn - (P(n- １)+...+p₁)  3.CPU使用率(CPUT Utilization)= CPU时间/总系统运行时长  4.Produce（產量）= 总完成的作业数 / 所需执行所花费的大致平均時間\n",
            "✅ Extracted Keywords: 刀劍神域 C8763\n",
            "✅ Core problem: 該代號「C8763」在原作中明確對應於黑乃的劍技。\n",
            "71 星光連流擊\n",
            "✅ Extracted Keywords: 斯卡羅  柴城\n",
            "✅ Core problem: 提取的核心問題是：劇中之地名「柴城」位於現今哪個行政區劃？\n",
            "72 屏東縣\n",
            "✅ Extracted Keywords: Google Colab A100 高級 GPU 訂閱制\n",
            "✅ Core problem: 使用A100高級GPU的需求\n",
            "73 $6.50/小時\n",
            "✅ Extracted Keywords: 機器學習 李宏毅 台大\n",
            "✅ Core problem: 李宏毅老師開設的機器學習課程\n",
            "74 李宏毅老師的機器學習課程是由國立台湾大学開設，目前有2025年春季班正在進行中。\n",
            "✅ Extracted Keywords: 國立臺灣大學 資工系 113 學年度 第2學期 書卷獎 獎金\n",
            "✅ Core problem: 雪江同學選擇第一個策略，至少要修多少分才不用簽減免申請書？\n",
            "75 財團法人聯詠科技教育基金會研究生獎學金\n",
            "✅ Extracted Keywords: Neuro-sama VTube Studio Live2D\n",
            "✅ Core problem: Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 哪個角色？\n",
            "76 Neuro-sama\n",
            "✅ Extracted Keywords: 從零開始的異世界生活 第三季  愛蜜莉雅 劫持者\n",
            "✅ Core problem: 從零開始的異世界生活第三季\n",
            "77 愛蜜莉雅方陣營：羅茲瓦爾宅邸、聖域、三英傑\n",
            "✅ Extracted Keywords: 海綿寶宝 失蹤記 刺破泡沫紅眼幇\n",
            "✅ Core problem: 提取的核心問題：海綿寶宝在第五季《失蹤記》中擊敗刺破泡沫紅眼幇。\n",
            "78 海綿寶宝在第五季《失蹤記》中擊敗刺破泡沫紅眼幇。\n",
            "✅ Extracted Keywords: 玉米 雙子葉\n",
            "✅ Core problem: 玉米是雙子葉植物。\n",
            "79 玉米是雙子葉植物。\n",
            "✅ Extracted Keywords: 中華民國 陸軍 軍歌\n",
            "✅ Core problem: 提取的核心問題：中華民國陸軍军歌前六字為何？\n",
            "80 風雲起，山河動\n",
            "✅ Extracted Keywords: 電資學院  資訊工程系\n",
            "✅ Core problem: 台大的電資學院哪個系規定物理、化學以及生物科目可以只擇一修習即可？\n",
            "81 資訊工程學系\n",
            "✅ Extracted Keywords: 月球背面 Sinus Amoris Lacu Oblivionis\n",
            "✅ Core problem: 憂傷湖（Lacus Doloris）、死lake （不確定是否為 Lacus Morti s） 、忘 lake  ( 不確認 )、恐怖Lake   ，以及愛灣。\n",
            "82 Ave Mujica\n",
            "✅ Extracted Keywords: 貝多芬 C♯小調第14號鋼琴奏鳴曲\n",
            "✅ Core problem: 《C♯小調第14號鋼琴奏鳴曲》\n",
            "83 C♯小調第14號鋼琴奏鳴曲\n",
            "✅ Extracted Keywords: 阿米斯音樂節\n",
            "✅ Core problem: 阿米斯音樂節\n",
            "84 阿米斯音樂節\n",
            "✅ Extracted Keywords: Poppy Playtime Chapter 4 黏土人\n",
            "✅ Core problem: 黏土人名字\n",
            "85 波比的游戏时间第四章是Poppy Playtime Chapter 4的一部分。\n",
            "✅ Extracted Keywords: 賓茂部落 Djumulj 太麻里鄉 排灣族語\n",
            "✅ Core problem: 賓茂村屬於何一行政區劃？\n",
            "86 金峰鄉\n",
            "✅ Extracted Keywords: 義大利 文藝復興 時期 米開朗基羅 大衛 雕像 佛拉倫斯\n",
            "✅ Core problem: 提取核心問題：米開朗基羅創作的《大衛》雕像最初是在何處？\n",
            "87 佛羅倫斯\n",
            "✅ Extracted Keywords: 中華民國 國軍 軍階 特級上將 蔣 中正\n",
            "✅ Core problem: 提取的核心問題：除了蔣中正之外，曾短暫晉升特級上將的是誰？\n",
            "88 蔣中正\n",
            "✅ Extracted Keywords: 英雄聯盟 2012 年 第二 賽季 世界 大賽\n",
            "✅ Core problem: 提取的核心問題：2012年第二賽季世界大赛「英雄聯盟」的總冠軍是哪一個戰隊？\n",
            "89 聯賽冠軍：天下無敵\n",
            "✅ Extracted Keywords: 日本麻將 非莊家 手牌 數量\n",
            "✅ Core problem: 提取的核心問題：在日本麻將中，非莊家一開始的手牌有幾張？\n",
            "90 日本麻將中，非莊家一開始的手牌有13張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "_TvF7tP8IJ1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}